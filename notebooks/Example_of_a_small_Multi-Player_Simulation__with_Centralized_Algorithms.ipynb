{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#An-example-of-a-small-Multi-Player-simulation,-with-Centralized-Algorithms\" data-toc-modified-id=\"An-example-of-a-small-Multi-Player-simulation,-with-Centralized-Algorithms-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>An example of a small Multi-Player simulation, with Centralized Algorithms</a></div><div class=\"lev2 toc-item\"><a href=\"#Creating-the-problem\" data-toc-modified-id=\"Creating-the-problem-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Creating the problem</a></div><div class=\"lev3 toc-item\"><a href=\"#Parameters-for-the-simulation\" data-toc-modified-id=\"Parameters-for-the-simulation-111\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Parameters for the simulation</a></div><div class=\"lev3 toc-item\"><a href=\"#Three-MAB-problems-with-Bernoulli-arms\" data-toc-modified-id=\"Three-MAB-problems-with-Bernoulli-arms-112\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Three MAB problems with Bernoulli arms</a></div><div class=\"lev3 toc-item\"><a href=\"#Some-RL-algorithms\" data-toc-modified-id=\"Some-RL-algorithms-113\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Some RL algorithms</a></div><div class=\"lev2 toc-item\"><a href=\"#Creating-the-EvaluatorMultiPlayers-objects\" data-toc-modified-id=\"Creating-the-EvaluatorMultiPlayers-objects-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Creating the <code>EvaluatorMultiPlayers</code> objects</a></div><div class=\"lev2 toc-item\"><a href=\"#Solving-the-problem\" data-toc-modified-id=\"Solving-the-problem-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Solving the problem</a></div><div class=\"lev2 toc-item\"><a href=\"#Plotting-the-results\" data-toc-modified-id=\"Plotting-the-results-14\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Plotting the results</a></div><div class=\"lev3 toc-item\"><a href=\"#First-problem\" data-toc-modified-id=\"First-problem-141\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>First problem</a></div><div class=\"lev3 toc-item\"><a href=\"#Second-problem\" data-toc-modified-id=\"Second-problem-142\"><span class=\"toc-item-num\">1.4.2&nbsp;&nbsp;</span>Second problem</a></div><div class=\"lev3 toc-item\"><a href=\"#Third-problem\" data-toc-modified-id=\"Third-problem-143\"><span class=\"toc-item-num\">1.4.3&nbsp;&nbsp;</span>Third problem</a></div><div class=\"lev3 toc-item\"><a href=\"#Comparing-their-performances\" data-toc-modified-id=\"Comparing-their-performances-144\"><span class=\"toc-item-num\">1.4.4&nbsp;&nbsp;</span>Comparing their performances</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# An example of a small Multi-Player simulation, with Centralized Algorithms\n",
    "\n",
    "First, be sure to be in the main folder, or to have [SMPyBandits](https://github.com/SMPyBandits/SMPyBandits) installed, and import `EvaluatorMultiPlayers` from `Environment` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "!pip install SMPyBandits watermark\n",
    "%load_ext watermark\n",
    "%watermark -v -m -p SMPyBandits -a \"Lilian Besson\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "# Local imports\n",
    "from SMPyBandits.Environment import EvaluatorMultiPlayers, tqdm"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need arms, for instance `Bernoulli`-distributed arm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Import arms\n",
    "from SMPyBandits.Arms import Bernoulli"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we need some single-player and multi-player Reinforcement Learning algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "# Import algorithms\n",
    "from SMPyBandits.Policies import *\n",
    "from SMPyBandits.PoliciesMultiPlayers import *"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "source": [
    "# Just improving the ?? in Jupyter. Thanks to https://nbviewer.jupyter.org/gist/minrk/7715212\n",
    "from __future__ import print_function\n",
    "from IPython.core import page\n",
    "def myprint(s):\n",
    "    try:\n",
    "        print(s['text/plain'])\n",
    "    except (KeyError, TypeError):\n",
    "        print(s)\n",
    "page.page = myprint"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, this imported the `UCB` algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "UCBalpha?"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as the `CentralizedMultiplePlay` multi-player policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "source": [
    "CentralizedMultiplePlay?"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a collision model. The usual ones are defined in the `CollisionModels` package, and the only one we need is the classical one, where two or more colliding users don't receive any rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "# Collision Models\n",
    "from SMPyBandits.Environment.CollisionModels import onlyUniqUserGetsReward\n",
    "\n",
    "onlyUniqUserGetsReward?"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Creating the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters for the simulation\n",
    "- $T = 10000$ is the time horizon,\n",
    "- $N = 100$ is the number of repetitions (should be larger to have consistent results),\n",
    "- $M = 2$ is the number of players,\n",
    "- `N_JOBS = 4` is the number of cores used to parallelize the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "HORIZON = 10000\n",
    "REPETITIONS = 100\n",
    "NB_PLAYERS = 2\n",
    "N_JOBS = 4\n",
    "collisionModel = onlyUniqUserGetsReward"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three MAB problems with Bernoulli arms\n",
    "We consider in this example $3$ problems, with `Bernoulli` arms, of different means.\n",
    "\n",
    "1. The first problem is very easy, with two good arms and three arms, with a fixed gap $\\Delta = \\max_{\\mu_i \\neq \\mu_j}(\\mu_{i} - \\mu_{j}) = 0.1$.\n",
    "2. The second problem is as easier, with a larger gap.\n",
    "3. Third problem is harder, with a smaller gap, and a very large difference between the two optimal arms and the suboptimal arms.\n",
    "\n",
    "> Note: right now, the multi-environments evaluator does not work well for MP policies, if there is a number different of arms in the scenarios. So I use the same number of arms in all the problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "ENVIRONMENTS = [  # 1)  Bernoulli arms\n",
    "        {   # Scenario 1 from [Komiyama, Honda, Nakagawa, 2016, arXiv 1506.00779]\n",
    "            \"arm_type\": Bernoulli,\n",
    "            \"params\": [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "        },\n",
    "        {   # Classical scenario\n",
    "             \"arm_type\": Bernoulli,\n",
    "             \"params\": [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "        },\n",
    "        {   # Harder scenario\n",
    "             \"arm_type\": Bernoulli,\n",
    "             \"params\": [0.005, 0.01, 0.015, 0.84, 0.85]\n",
    "        }\n",
    "    ]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some RL algorithms\n",
    "We will compare Thompson Sampling against $\\mathrm{UCB}_1$, using two different centralized policy:\n",
    "\n",
    "1. `CentralizedMultiplePlay` is the naive use of a Bandit algorithm for Multi-Player decision making: at every step, the internal decision making process is used to determine not $1$ arm but $M$ to sample. For UCB-like algorithm, the decision making is based on a $\\arg\\max$ on UCB-like indexes, usually of the form $I_j(t) = X_j(t) + B_j(t)$, where $X_j(t) = \\hat{\\mu_j}(t) = \\sum_{\\tau \\leq t} r_j(\\tau) / N_j(t)$ is the empirical mean of arm $j$, and $B_j(t)$ is a bias term, of the form $B_j(t) = \\sqrt{\\frac{\\alpha \\log(t)}{2 N_j(t)}}$.\n",
    "\n",
    "2. `CentralizedIMP` is very similar, but instead of following the internal decision making for all the decisions, the system uses just the empirical means $X_j(t)$ to determine $M-1$ arms to sample, and the bias-corrected term (i.e., the internal decision making, can be sampling from a Bayesian posterior for instance) is used just for one decision. It is an heuristic, proposed in [[Komiyama, Honda, Nakagawa, 2016]](https://arxiv.org/abs/1506.00779)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "nbArms = len(ENVIRONMENTS[0]['params'])\n",
    "assert all(len(env['params']) == nbArms for env in ENVIRONMENTS), \"Error: not yet support if different environments have different nb of arms\"\n",
    "nbArms\n",
    "\n",
    "SUCCESSIVE_PLAYERS = [\n",
    "    CentralizedMultiplePlay(NB_PLAYERS, nbArms, UCBalpha, alpha=1).children,\n",
    "    CentralizedIMP(NB_PLAYERS, nbArms, UCBalpha, alpha=1).children,\n",
    "    CentralizedMultiplePlay(NB_PLAYERS, nbArms, Thompson).children,\n",
    "    CentralizedIMP(NB_PLAYERS, nbArms, Thompson).children\n",
    "]\n",
    "SUCCESSIVE_PLAYERS"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mother class in this case does all the job here, as we use centralized learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "source": [
    "OnePlayer = SUCCESSIVE_PLAYERS[0][0]\n",
    "OnePlayer.nbArms\n",
    "\n",
    "OneMother = OnePlayer.mother\n",
    "OneMother\n",
    "OneMother.nbArms"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete configuration for the problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "source": [
    "configuration = {\n",
    "    # --- Duration of the experiment\n",
    "    \"horizon\": HORIZON,\n",
    "    # --- Number of repetition of the experiment (to have an average)\n",
    "    \"repetitions\": REPETITIONS,\n",
    "    # --- Parameters for the use of joblib.Parallel\n",
    "    \"n_jobs\": N_JOBS,    # = nb of CPU cores\n",
    "    \"verbosity\": 6,      # Max joblib verbosity\n",
    "    # --- Collision model\n",
    "    \"collisionModel\": onlyUniqUserGetsReward,\n",
    "    # --- Arms\n",
    "    \"environment\": ENVIRONMENTS,\n",
    "    # --- Algorithms\n",
    "    \"successive_players\": SUCCESSIVE_PLAYERS,\n",
    "}"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Creating the `EvaluatorMultiPlayers` objects\n",
    "We will need to create several objects, as the simulation first runs one policy against each environment, and then aggregate them to compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "%%time\n",
    "N_players = len(configuration[\"successive_players\"])\n",
    "\n",
    "# List to keep all the EvaluatorMultiPlayers objects\n",
    "evs = [None] * N_players\n",
    "evaluators = [[None] * N_players] * len(configuration[\"environment\"])\n",
    "\n",
    "for playersId, players in tqdm(enumerate(configuration[\"successive_players\"]), desc=\"Creating\"):\n",
    "    print(\"\\n\\nConsidering the list of players :\\n\", players)\n",
    "    conf = configuration.copy()\n",
    "    conf['players'] = players\n",
    "    evs[playersId] = EvaluatorMultiPlayers(conf)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Solving the problem\n",
    "Now we can simulate the $2$ environments, for the successive policies. That part can take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "source": [
    "%%time\n",
    "for playersId, evaluation in tqdm(enumerate(evs), desc=\"Policies\"):\n",
    "    for envId, env in tqdm(enumerate(evaluation.envs), desc=\"Problems\"):\n",
    "        # Evaluate just that env\n",
    "        evaluation.startOneEnv(envId, env)\n",
    "        # Storing it after simulation is done\n",
    "        evaluators[envId][playersId] = evaluation"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the results\n",
    "And finally, visualize them, with the plotting method of a `EvaluatorMultiPlayers` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "def plotAll(evaluation, envId):\n",
    "    evaluation.printFinalRanking(envId)\n",
    "    # Rewards\n",
    "    evaluation.plotRewards(envId)\n",
    "    # Fairness\n",
    "    #evaluation.plotFairness(envId, fairness=\"STD\")\n",
    "    # Centralized regret\n",
    "    evaluation.plotRegretCentralized(envId, subTerms=True)\n",
    "    #evaluation.plotRegretCentralized(envId, semilogx=True, subTerms=True)\n",
    "    # Number of switches\n",
    "    #evaluation.plotNbSwitchs(envId, cumulated=False)\n",
    "    evaluation.plotNbSwitchs(envId, cumulated=True)\n",
    "    # Frequency of selection of the best arms\n",
    "    evaluation.plotBestArmPulls(envId)\n",
    "    # Number of collisions - not for Centralized* policies\n",
    "    #evaluation.plotNbCollisions(envId, cumulated=False)\n",
    "    #evaluation.plotNbCollisions(envId, cumulated=True)\n",
    "    # Frequency of collision in each arm\n",
    "    #evaluation.plotFrequencyCollisions(envId, piechart=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (12.4, 7)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First problem\n",
    "$\\mu = [0.3, 0.4, 0.5, 0.6, 0.7]$ was an easy Bernoulli problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "source": [
    "for playersId in tqdm(range(len(evs)), desc=\"Policies\"):\n",
    "    evaluation = evaluators[0][playersId]\n",
    "    plotAll(evaluation, 0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second problem\n",
    "$\\mu = [0.1, 0.3, 0.5, 0.7, 0.9]$ was an easier Bernoulli problem, with larger gap $\\Delta = 0.2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "source": [
    "for playersId in tqdm(range(len(evs)), desc=\"Policies\"):\n",
    "    evaluation = evaluators[1][playersId]\n",
    "    plotAll(evaluation, 1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third problem\n",
    "$\\mu = [0.005, 0.01, 0.015, 0.84, 0.85]$ is an harder Bernoulli problem, as there is a huge gap between suboptimal and optimal arms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "source": [
    "for playersId in tqdm(range(len(evs)), desc=\"Policies\"):\n",
    "    evaluation = evaluators[2][playersId]\n",
    "    plotAll(evaluation, 2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Comparing their performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "source": [
    "def plotCombined(e0, eothers, envId):\n",
    "    # Centralized regret\n",
    "    e0.plotRegretCentralized(envId, evaluators=eothers)\n",
    "    # Fairness\n",
    "    e0.plotFairness(envId, fairness=\"STD\", evaluators=eothers)\n",
    "    # Number of switches\n",
    "    e0.plotNbSwitchsCentralized(envId, cumulated=True, evaluators=eothers)\n",
    "    # Number of collisions - not for Centralized* policies\n",
    "    #e0.plotNbCollisions(envId, cumulated=True, evaluators=eothers)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "source": [
    " N = len(configuration[\"environment\"])\n",
    "for envId, env in enumerate(configuration[\"environment\"]):\n",
    "    e0, eothers = evaluators[envId][0], evaluators[envId][1:]\n",
    "    plotCombined(e0, eothers, envId)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "> That's it for this demo!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "notify_time": "5",
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "150px",
    "width": "251px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "538px",
    "left": "0px",
    "right": "1388px",
    "top": "208px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "state": {
    "0b963f8afd7d4f42bf0e582d6b5e89f4": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "1a4b0bece4d64d74927a933db20e9938": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "1cbcadb0904a45e2b392c59c1393acd5": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "1d35741114044a5492eb0baa872afee2": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "1f7ef6227d624bd18a35d48d7bc977e0": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "27a73e37c97844319df43ba61f169796": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "3599eb2687c54d6c89b16029b3d707ff": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "371b9ef0df1b48d2a25ea7130356fc8a": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "3ea18491967c4a4fb1c6dc39729935f3": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "54af7c32d75242418882c1990543f5fe": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "6331359b71f04d89a527dd63233949b6": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "635fd90668b44b3bbce7c542d159b90c": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "6fe3ebc587f24115bbeac3cf26a1ffa8": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "80bfeda813394e6abdb93efcbbde4d25": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "897383c833434192baa4718be9f537fd": {
     "views": [
      {
       "cell_index": 35
      }
     ]
    },
    "8ee5a29fdaba4c009d8e813ebe68571f": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "90281ff8221f40e99277cddef760fb0e": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "a8b8912617f044d9a6136f9a47dc7f38": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "b19363a776014b4d8bd007616036f772": {
     "views": [
      {
       "cell_index": 33
      }
     ]
    },
    "c2c965aedc69411e93ef2e37965e0621": {
     "views": [
      {
       "cell_index": 27
      }
     ]
    },
    "cdaca6e72c9a43429cf60eefebf01add": {
     "views": [
      {
       "cell_index": 37
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
